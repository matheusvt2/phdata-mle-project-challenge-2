[Understandment]
- Understandment o create_model.py it reads kc_house_data.csv and zipcode_demographics.csv
- Found bugs in code in despite of data paths.
- Merge demographics and data dataframes, use "price" as target variable.
- Uses holdout to split data and train a KNeighborsRegressor, but don't use any validation to validate the model
-> Outputs: 
    - "model.pkl" Model
    - "model_features.json" Features (Columns)

```3. Evaluate the performance of the model. You should start with the code in create_model.py and try to figure out how well the model will generalize
to new data. Has the model appropriately fit the dataset?```
- To get the training score, need to implement a validation into create_model.py
- I'll implement a simple holdout validation, I could use a Cross Validation but it is a little more complex and time consuming

-> Output:
    - "metrics.json" -> {
                        "mse": 40671000467.84013,
                        "rmse": 201670.52453901173,
                        "r2": 0.728112531681446
                        }
This metrics shows a model that is not well fitted to the dataset, based on this validation, R2 =0.7 indicates a moderated explanability. 
Adding RMSE to analisys reinterates that the model is diverging on price inference by 201.607 units. This is a big value.

-> Recommendations:
    - Outliers analisys (knn could suffer from outliers, depending on n_neighbors(k)) 
    - Density analisys
        - We are using KNeighborsRegressor, the density is important to better fitting.
        - Sparse data leads to overfitting whereas high density could underfit the model.
    - Cross Validation (removes bias on validation, also could use to choose a new n_neighbors value)
    - Feature Importance selection or PCA technique (low dimensionality improves knn)
    - Choose best parameters with Grid Search / Random Search

[Model Training]
- I'll create a dockerfile to run create_model.py to save locally the model generated by the file
- Outputs are saved on /model and i'll use to create the API
- PS: Using the dataset together with application IS not best practice but for the sake of this project I'll put all together


[API Building]
- I like to create the following folder structure for APIs (based on but not strictly SOLID)
    app/
      api/
        routes/
        models/
      services/
      config/
      utils/
      data/

- First I will create de Models for this API using Pydantic based on:
    - "The inputs to this endpoint should be the columns in `data/future_unseen_examples.csv`."
    - "The endpoint should return a JSON object with a prediction from the model, as well as any metadata you see as necessary."
    - "Bonus: the basic model only uses a subset of the columns provided in the house sales data. Create an additional API endpoint where only the required features have to be provided in order to get a prediction."
- Then I'll create the routes for the API and update main.py with theese routes
- Finally, I will create the service responsible for:
    - augment input payload with zipcode_demographics
    - predict the price
    
[Model Testing]
- I will create a code to test the client "test_client.py" to 
    "create a test script which submits examples to the endpoint to demonstrate its behavior".
    - Test Batch /predict (expect PASS)
    - Test InLine /predict/minimal (expect PASS)
- I will create a csv from future_unseen_examples using only the "Minimal" fileds to test both APIs
    - Test Batch /predict (expect FAIL)
    - Test InLine /predict/minimal (expect PASS)

[Model Evaluation]
- I will save all prediction to a .csv file, mantaining the input format from ("future_unseen_examples.csv").
- Populate values from prediction to a column named price_prediction
- Add two more columns prediction_timestamp and endpoint_type
- Since is a supervisioned model, it needs the ground truth value to validate   
    - Add an empty column name price_gt (ground truth)
- Programatically edit this collumn name simulating a manual or automated labeling
- Compare prod and dev metrics